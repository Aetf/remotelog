- [DONE] Is reording causing failures
    * increase reording buffer
- [DONE] Maximum un-acked, un-failed frames -> congestion window

- [DONE] Stamp at spout ack
- [DONE] timing fat_features opeartions separately
- [DONE] split fat_features into indivitual bolts

- [DONE] measure CPU utilization
- [DONE] queueing size at each stage
    * [DONE] with different max pending number: 128, 64, 96
- same vs. different node
    * change fat_features parallelism 52->25
    * Track how many frames goes from scale to fat_features on different machine
- [DONE] try scale parallelism at 2

- Analysis
    * [DONE] stage sorting orders breaks when using string value for event stages

- [DONE] Try max pending = 1 to show pure service time
    * [DONE] explain the fps graph: streamreader was paused when spout is paused
    * [DONE] explain the latency graph
- Reason for periodic failures
    * spout emits many frames in one second, so these frames fail in one second later
- Effect of frameskip
    * [DONE] use frameskip on SpoutOnly
    * use frameskip on DNNTopology

- [DONE] Raw services time: DNNTopology (CPU, GPU)
- ~~Make GPU working on Storm~~

- [DONE] Why not scale

- [DONE] fix cores, find maximum fps with different parallelism hints
    - 8 cores
    - 16 cores
    - 32 cores
- [DONE] 1 FPS with only 1 cores: breaks

- [DONE] Fix timing
    * less than 5 frames are dropped now (out of ~3000)
- [DONE] Better FPS control
    * can achieve 30+ FPS steadily under heavy load
- Run over network on 2 nodes
    * [DONE] fix cores, max fps with different parallelism hints with latency constrain
    * [DONE] service time
    * fix fps and cores, latency with different parallelism hints
        + 2.5s latency

- [DONE] !!! low workload for 8 core, 16 core, 32 core, 32+32 core
- [DONE] !!! fix latency 2.5s, for 16 core, 32 core, 32+32 core
    * [DONE] 16 cores, 7 FPS, fat > 32
    * [DONE] 32 cores, FPS < 8, fat = range(27, 33)
    * [DONE] 32+32 cores, scale=1, fat = [40,50,60,80,100], FPS = [14,15,16]
    * [DONE] 8 cores, fat > 12, FPS = [3, 4]
    * [DONE] re-run 32+32 cores, scale=2 for FPS>11
- [DONE] Organize work into notebook
- [DONE] verify if frames sit on the same machine
- [DONE] 32 cores 7 FPS    SMT case?

- [DONE] check break-downs for each case
- reason for 4s latency
    * try change nice value for fat_features thread
    * ask Parker about the variation in computation time

- [DONE] Get more reliable data from GPU flavor
- Get closed captioning work with caffe

- [DONE] verify one core supports 10FPS
- [DONE] try batch size from 1-5 to verify batching actually improves speed: possibly no imporvement

- [DONE] fix out of order Ack (maybe sliending window issue)

- [DONE] variation of computation time for obj tracking using img or video

- [DONE] obj tracking data

- dynamic batching

- [DONE] if it's easy to break down algorithm

- [DONE] verify if obj tracking and gpu_classifycation runs at 30FPS

- problems
    * [DONE] why the queue before streamer keeps growing even though streamer takes very little time?
    * unusuall observation with 1 GPU
    * [DONE] sometimes the output FPS can be very smooth and thus can achieve a very low latency

- Run exp for SimpleBatcher
- [DONE] If previous ForwardNetCaffe::forward implementation flawed?
- [DONE] Add CaptionerOp: output blob is "fc7"
- [DONE] Run exp for S2VT
- Verify storm is issuing tuples out of order
    * [DONE] spout(A) -> noop(A): none
    * spout(A) -> noop(B)
    * spout(A) -> scale(B) -> noop(A)
    * spout(A) -> scale(A) -> noop(B)

- [DONE] Fixed group-size for captioning

- [DONE] Dataset: VOT2015: 1 FPS 8 cores GPU

- [DONE] Check object tracking code

- Recalculate latency for captioner

- 3D Matrixes: Multiple streams, multiple workload, FPS

- Fork the repo

- [DONE] Define the query
    * ImC: frames
    * Cap: scene
    * ObT: frame

- OBT on DNN (3 days)

- Investigate storm queuing (3 days)

- Latency data for ObT (1 days)

- Variation: avg and 99% value, cdf
    * Captioning (2 days)
    * ObT (1 day)
    * [DONE] ImC: OpenCV::DNN, verify if caffe (GPU) (1 day)

- Plot CDF and Avg and 99th Percential value for 10 runs
    * ImC
    * ObT
    * Cap

- Found
    * A strange problem: in 2016-7-14/dnn_classification_batch-10, the output FPS (28) is far below input FPS (43), but still the latency doesn't queue up. This happens for all exps with batch-size>2.

### new workload

- motion classification
- [DONE] object tracking
- captioning (motivation?): video accessibility
- searching
    * panopto: search substitles and words on screen (OCR)


##### Some questions
 - The difference between video streaming and normal streaming which typically involves logs and other form of text.
- From the scheduling perspective, what the workload actually does doesn't matter, as long as the workload can be classified into a certain category, i.e. computation intensive, memory intensive, or network I/O intensive.
    * Therefore, what is the streaming video application specific effects on the scheduling strategy?

##### Related papers
- [Large-scale Video Classification with Convolutional Neural Networks](http://cs.stanford.edu/people/karpathy/deepvideo/)
- [Motion pattern-based video classification and retrieval](http://dl.acm.org/citation.cfm?id=1283278)
- [Motion recovery for video content classification](http://dl.acm.org/citation.cfm?id=211433)

##### Related resources
- conferences
    * vision
        + CVPR
        + ICCV
    * System
        + ICDCS
    * ASPLOS (targeting)

- scheduling multiple topology at the same time

- storm benchmark specific to streaming videos
    * an end-to-end video streaming analysis applicaotin
    * Yahoo! benchmarks
    * Yahoo! CaffeOnSpark
- intel-hadoop/storm-benchmark
    * simple topology

- GPU batching
- [Realword Storm application](http://storm.apache.org/releases/current/Powered-By.html)
- [A list of Storm benchmarks and other resources](https://github.com/manuzhang/awesome-streaming#benchmark)
- [A great article about streaming](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101)
